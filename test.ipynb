{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ef9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7d58bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain, plotly and Chroma\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598996a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8594ff-9f58-4de0-9dd3-062fc4524de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "os.environ[\"OPEN_API_KEY\"] = os.getenv(\"OPEN_API_KEY\")\n",
    "print(os.getenv(\"OPEN_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f9825-5275-4c4a-9fb4-60568acbd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING THE DOCUMENTS\n",
    "folders = glob.glob(\"test_documents/*\")\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.*\", loader_cls=PyPDFLoader)\n",
    "    folder_docs = loader.load()\n",
    "    print(f\"Loading {doc_type} documents...\")\n",
    "    print(f\"Found {len(folder_docs)} documents in {doc_type} folder.\")\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775386d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total documents loaded: {len(documents)}\")\n",
    "print(documents[4].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING THE DOCUMENTS\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) # initialized a splitter\n",
    "chunks = text_splitter.split_documents(documents)  # split the documents into smaller chunks\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(chunks[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=\"\" + os.getenv(\"OPEN_API_KEY\"), #creating a embedding function using OpenAI API key\n",
    ")\n",
    "if os.path.exists(db_name):\n",
    "    print(f\"Loading existing vector store from {db_name}...\")\n",
    "    vector_store = Chroma(persist_directory=db_name, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81fd826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating a vector store\n",
    "vector_store = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vector store created with {len(vector_store)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = vector_store._collection\n",
    "sample = collection.get(limit=1, include=[\"embeddings\", \"metadatas\"])[\"embeddings\"][0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc105b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let try to see if we can visualize the embeddings\n",
    "\n",
    "result = collection.get(include=[\"embeddings\", \"metadatas\", \"documents\"])\n",
    "vectors = np.array(result['embeddings'])\n",
    "\n",
    "documents = result['documents']\n",
    "\n",
    "# print(f\"Total vectors retrieved: {len(vectors)}\")\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "source_types = [metadata['source'] for metadata in result['metadatas']]\n",
    "\n",
    "\n",
    "#assigniing colors based on document types\n",
    "colors = [['red', 'blue'][['judgements', 'primary_sources'].index(t)] for t in doc_types]\n",
    "print(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0dec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(source_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "71c729ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "which amounts to deficiency in service as the delivery was not as per the promises made by the\n",
      "opposite parties. This is also the violation of policy of Amazon of Free Open Box delivery, which\n",
      "includes the Open Box Inspection at the time of delivery to ensure the delivery of same product in\n",
      "proper order. This service is available on pre-paid schedule delivery orders for TV, microwave,\n",
      "washing machine and refrigerator products. In case the product found to be defective or otherwise\n",
      "damage, the customer can refused to take the delivery can refund will be credited to his account. The\n",
      "order of the complainant was also covered under the same policy having 10 days time to return the\n",
      "item but despite that the delivery was not taken back and the complainant was informed by the\n",
      "executive of opposite party No.1 that the complainant was not eligible to the OAKTER Smart Home\n",
      "Kit and he has not clicked the same at the time of purchase of TV. It is pertinent to mention here\n",
      "\n",
      "Result 2:\n",
      "through opposite party No.1. There was a scheme of OAKTER Smart Home Kit which was to be\n",
      "delivered free of cost with the TV. The complainant received invoice dated 02.10.2019 on his email,\n",
      "whereby it was mentioned that the TV is 43\" in size and was to be delivered along with OAKTER\n",
      "Smart Home Kit Offer. On delivering of the TV, the complainant was surprised to see that the TV\n",
      "supplied was only of 38\" instead of 43\" and OAKTER Smarthome Kit was also not supplied as\n",
      "committed by the opposite parties. On this, the complainant asked the opposite parties for\n",
      "replacement of TV with immediate effect. He even called the customer care helpline number of\n",
      "Amazon and requested them to take back the product but their officials refused to return the TV,\n",
      "which amounts to deficiency in service as the delivery was not as per the promises made by the\n",
      "opposite parties. This is also the violation of policy of Amazon of Free Open Box delivery, which\n",
      "\n",
      "Result 3:\n",
      "Hence, whether the product is defective or not will only be decided by the Manufacturing Company\n",
      "or its Authorized Service Centre. The returned product would be sent to the Manufacturing\n",
      "Company for detection of defect. Hence, no replacement order was created for the customer. As on\n",
      "August 12, 2015, the product was not received at the warehouse of the O.P. No.1. The complainant\n",
      "had sent a snapshot of the tracking information which showed it was still in transit. No tracking is\n",
      "available now on the website. The O.P. No.1 did not receive the product sent by the complainant,\n",
      "hence a refund was not issued. Replacement order was not created for the complainant, therefore,\n",
      "replacement of the product would not be possible. The complaint filed by the complainant is\n",
      "completely vague and is debarred of any merits as the complainant is trying to mislead the District\n",
      "Forum by presenting prompt service to its customer and hence would not be held guilty of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what should i do if a package that i got deliverred was an empty box?\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Result {i+1}:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ab37aec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL, api_key=os.getenv(\"OPEN_API_KEY\"))\n",
    "\n",
    "# Alternative - if you'd like to use Ollama locally, uncomment this line instead\n",
    "# llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\n",
    "\n",
    "query = \"what should i do if a package that i got deliverred was an empty box?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d15eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/2ff82h_n6076ds23_xr8_9q40000gn/T/ipykernel_21277/2365268377.py:10: LangChainDeprecationWarning:\n",
      "\n",
      "The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "\n",
      "/var/folders/qw/2ff82h_n6076ds23_xr8_9q40000gn/T/ipykernel_21277/2365268377.py:13: LangChainDeprecationWarning:\n",
      "\n",
      "This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "\n",
      "/var/folders/qw/2ff82h_n6076ds23_xr8_9q40000gn/T/ipykernel_21277/2365268377.py:22: LangChainDeprecationWarning:\n",
      "\n",
      "The class `ConversationalRetrievalChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~create_history_aware_retriever together with create_retrieval_chain (see example in docstring)` instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for ConversationalRetrievalChain\nquestion_generator\n  Field required [type=missing, input_value={'retriever': VectorStore...ecretStr('**********'))}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nllm\n  Extra inputs are not permitted [type=extra_forbidden, input_value=ChatOpenAI(client=<openai...SecretStr('**********')), input_type=ChatOpenAI]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m memory = ConversationBufferMemory(memory_key=\u001b[33m\"\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m\"\u001b[39m, return_messages=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 5. Now create the full ConversationalRetrievalChain\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m conversation_chain = \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 6. Run your query\u001b[39;00m\n\u001b[32m     30\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mPlease explain what refund rights I have if my online order was delayed.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:224\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    223\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 2 validation errors for ConversationalRetrievalChain\nquestion_generator\n  Field required [type=missing, input_value={'retriever': VectorStore...ecretStr('**********'))}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nllm\n  Extra inputs are not permitted [type=extra_forbidden, input_value=ChatOpenAI(client=<openai...SecretStr('**********')), input_type=ChatOpenAI]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
