# Digi-Vakeel: A Multi-Modal Legal Assistant

`digi-vakeel` is an advanced, multi-modal, and multi-lingual legal assistant powered by Retrieval-Augmented Generation (RAG). This project leverages a custom LLM fine-tuned on Indian legal data to provide accurate answers from a corpus of legal documents. It seamlessly handles user queries in both English and Malayalam, and can process information from text and images.

---

## Core Features

-   **Custom Fine-Tuned LLM:** Utilizes `digi-vakeel`, a `Llama-3.1-8B` model fine-tuned using Unsloth on the `definitelyarjun/In-Legal` dataset for specialized legal knowledge.
-   **Retrieval-Augmented Generation (RAG):** Answers questions based on a collection of Indian legal case documents stored in a local ChromaDB vector store.
-   **Vision-Enabled (OCR):** If an image is uploaded, it uses the `microsoft/trocr-large-printed` model to perform Optical Character Recognition (OCR), extracting text and combining it with the user's query for enhanced context.
-   **Multi-Lingual Support:** Natively handles queries in both **English** and **Malayalam**.
-   **Automatic Language Routing:** Intelligently detects the query language and routes the request through the appropriate processing chain. For Malayalam queries, it translates the query to English for the RAG pipeline and then translates the English answer back to Malayalam for the user.
-   **Interactive Web UI:** A clean and simple user interface built with Gradio for easy interaction.

---

## Tech Stack & Architecture

The application's logic is orchestrated by **LangChain**, using `RunnableBranch` to manage the complex routing between different processing pipelines.

-   **Core Framework:** LangChain
-   **LLM (Local):** `digi-vakeel` (fine-tuned Llama-3.1) served via **Ollama**.
-   **Vector Store:** **ChromaDB** for local document storage and retrieval.
-   **Embeddings:** `BAAI/bge-base-en-v1.5` from Hugging Face.
-   **Vision (OCR):** `microsoft/trocr-large-printed` via the Hugging Face `transformers` library.
-   **Translation:** **SarvamAI API**.
-   **Language Detection:** `langdetect` library.
-   **Web Interface:** **Gradio**.
-   **Credentials Management:** `python-dotenv` for secure API key handling.

The core of the application in `main.py` defines a `full_chain` that intelligently routes user input to one of four distinct processing pipelines based on the input's characteristics (presence of an image and language of the query).

---

## Setup and Installation

Follow these steps to get the project running locally.

### 1. Prerequisites
-   [Python 3.10+](https://www.python.org/downloads/)
-   [Git](https://git-scm.com/downloads/)
-   [Ollama](https://ollama.com/) installed and running.

### 2. Clone the Repository
```bash
git clone <your-repository-url>
cd digi-vakeel
```

### 3. Create and Activate a Virtual Environment
```bash
# Create the environment
python -m venv venv

# Activate it (on Windows PowerShell)
.\venv\Scripts\Activate

# On macOS/Linux
# source venv/bin/activate
```
### 4. Install Dependencies
```bash
pip install -r requirements.txt
```
### 5. Set Up Local Models (Ollama)
The model is not on Ollama Hub, you will need to create it from the GGUF file generated by the model_finetuning.ipynb notebook.

### 6. Configure Credentials
You need credentials for two services: Hugging Face (for the OCR model) and SarvamAI (for translation).

A. Hugging Face:
Log in to your Hugging Face account via the terminal. This stores an access token that the transformers library uses to download gated or private models like TrOCR.

```bash
huggingface-cli login
```

You will be prompted to paste an access token from your Hugging Face settings.

B. SarvamAI:
Create a file named .env in the root of your project directory. Add your SarvamAI API key to this file:

```bash
SARVAM_API_KEY="your_actual_sarvam_api_key_here"
```

The application will automatically load this key.

## Prepare Your Documents
This project is configured to read PDF files from a specific local directory.

Make sure the path C:/Users/arjun/OneDrive/Documents/digi-vakeel/datasets/rag_pdf in rag_implementation.py is updated to point to your desired folder.

Place all your PDF documents in that folder.
The first time you run the app, it will process these files and create a persistent vector store in the vector_store directory.

## Running the Application
Once all setup steps are complete, launch the Gradio web interface with the following command:

```bash
python proto_gradio.py
```

Open the local URL provided in the terminal (http://127.0.0.1:7860) in your web browser to interact with the application.

## Fine-Tuning Information

The core LLM, digi-vakeel, was fine-tuned specifically for this project. The process is detailed in the model_finetuning.ipynb notebook.


Base Model: unsloth/Meta-Llama-3.1 (8B)
Technique: QLoRA using the Unsloth library for memory-efficient fine-tuning.
Dataset: definitelyarjun/In-Legal, a custom dataset of Indian legal instructions and responses.
